# -*- coding: utf-8 -*-
"""Saliencey_Maps_+_MNIST_1D_Double_descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a3shedpV2F-PnVNkHe_Mj7cTLm-dsWdM

# **MNIST-1D**: Observing deep double descent

This notebook investigates double descent as described in section 8.4 of the ["Understanding Deep Learning"](https://udlbook.github.io/udlbook/) textbook.

The deep double descent phenomenon was [originally described here](https://arxiv.org/abs/1812.11118) and later extended to modern architectures and large datasets in an [OpenAI research project](https://openai.com/blog/deep-double-descent/).

This case study is meant to show the convenience and computational savings of working with the low-dimensional MNIST-1D dataset. You can find more details at https://github.com/greydanus/mnist1d.
"""

from google.colab import drive
drive.mount('/content/drive')

# Run this if you're in a Colab to make a local copy of the MNIST 1D repository
!git clone https://github.com/greydanus/mnist1d

import torch, torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from torch.optim.lr_scheduler import StepLR
import numpy as np
import matplotlib.pyplot as plt
import mnist1d
import random
random.seed(0)

# Try attaching to GPU -- Use "Change Runtime Type to change to GPUT"
DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
print('Using:', DEVICE)

import torch
import matplotlib.pyplot as plt

from mnist1d.data import get_templates, get_dataset_args, get_dataset
from mnist1d.train import get_model_args, train_model
from mnist1d.models import ConvBase, GRUBase, MLPBase, LinearBase
from mnist1d.utils import set_seed, plot_signals, ObjectView, from_pickle

import seaborn as sns

args = mnist1d.data.get_dataset_args()
args.num_samples = 8000 # try after changing this
args.train_split = 0.5 # try after changing this
args.corr_noise_scale = 0.25 # try after changing this
args.iid_noise_scale = 2e-2 # try after changing this
data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=True)

# Add 15% noise to training labels
for c_y in range(len(data['y'])):
    random_number = random.random()
    if random_number < 0.15 :
        random_int = int(random.random() * 10)
        data['y'][c_y] = random_int

# The training and test input and outputs are in
# data['x'], data['y'], data['x_test'], and data['y_test']
print("Examples in training set: {}".format(len(data['y'])))
print("Examples in test set: {}".format(len(data['y_test'])))
print("Length of each example: {}".format(data['x'].shape[-1]))

data['t']

data.keys()

for k in range(1,10):
    i = np.random.random_integers(0,100)
    samp_x = data['x'][i]
    t = data['t'].reshape((40,1))
    y = data['y']
    plt.plot(samp_x, t, 'k-', linewidth=2)
    print(y[i])
    plt.show()
    #fig = plot_signals(data['x'], t, labels = y, ratio=3, dark_mode=False)

# Initialize the parameters with He initialization
def weights_init(layer_in):
  if isinstance(layer_in, nn.Linear):
    nn.init.kaiming_uniform_(layer_in.weight)
    layer_in.bias.data.fill_(0.0)

# Return an initialized model with two hidden layers and n_hidden hidden units at each
def get_model(n_hidden):

  D_i = 40    # Input dimensions
  D_k = n_hidden   # Hidden dimensions
  D_o = 10    # Output dimensions

  # Define a model with two hidden layers of size 100
  # And ReLU activations between them
  model = nn.Sequential(
  nn.Linear(D_i, D_k),
  nn.ReLU(),
  nn.Linear(D_k, D_k),
  nn.ReLU(),
  nn.Linear(D_k, D_o))

  # Call the function you just defined
  model.apply(weights_init)

  # Return the model
  return model ;

def fit_model(model, data):

  # choose cross entropy loss function (equation 5.24)
  loss_function = torch.nn.CrossEntropyLoss()
  # construct SGD optimizer and initialize learning rate and momentum
  # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
  optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)


  # create 100 dummy data points and store in data loader class
  x_train = torch.tensor(data['x'].astype('float32'))
  y_train = torch.tensor(data['y'].transpose().astype('long'))
  x_test= torch.tensor(data['x_test'].astype('float32'))
  y_test = torch.tensor(data['y_test'].astype('long'))

  # load the data into a class that creates the batches
  data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))

  # loop over the dataset n_epoch times
  n_epoch = 1000

  for epoch in range(n_epoch):
    # loop over batches
    for i, batch in enumerate(data_loader):
      # retrieve inputs and labels for this batch
      x_batch, y_batch = batch
      # zero the parameter gradients
      optimizer.zero_grad()
      # forward pass -- calculate model output
      pred = model(x_batch)
      # compute the loss
      loss = loss_function(pred, y_batch)
      # backward pass
      loss.backward()
      # SGD update
      optimizer.step()

    # Run whole dataset to get statistics -- normally wouldn't do this
    pred_train = model(x_train)
    pred_test = model(x_test)
    _, predicted_train_class = torch.max(pred_train.data, 1)
    _, predicted_test_class = torch.max(pred_test.data, 1)
    errors_train = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)
    errors_test= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)
    losses_train = loss_function(pred_train, y_train).item()
    losses_test= loss_function(pred_test, y_test).item()
    if epoch%100 ==0 :
      print(f'Epoch {epoch:5d}, train loss {losses_train:.6f}, train error {errors_train:3.2f},  test loss {losses_test:.6f}, test error {errors_test:3.2f}')

  return errors_train, errors_test, predicted_train_class, predicted_test_class

# %rm -rf <"/content/drive/MyDrive/Saliency_Map_MNIST_results/CP/">

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import matplotlib.pyplot as plt
# import numpy as np
# import torch
# from torch.utils.data import DataLoader, TensorDataset
# 
# # hidden_variables = np.array([600,700,800,900])
# hidden_variables = np.array([2,4,6,8,10,14,18,22,26,30,35,40,45,50,55,60,70,80,90,100,120,140,160,180,200,250,300,400, 500, 600, 700, 800, 900]);
# errors_train_all = np.zeros_like(hidden_variables)
# errors_test_all = np.zeros_like(hidden_variables)
# 
# # Assume get_model() and fit_model() are defined elsewhere
# 
# for c_hidden in range(0, len(hidden_variables)):
#     print("#"*100)
#     print(f'Training model with {hidden_variables[c_hidden]:3d} hidden variables')
#     model = get_model(hidden_variables[c_hidden])
#     errors_train, errors_test, pred_train, pred_test = fit_model(model, data)
# 
#     errors_train_all[c_hidden] = errors_train
#     errors_test_all[c_hidden] = errors_test
# 
#     x_test = torch.tensor(data['x_test'].astype('float32'), requires_grad=True)
#     y_test = torch.tensor(data['y_test'].transpose().astype('long'))
#     test_data_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=12, shuffle=False)
# 
# 
#     for x_batch, y_d in test_data_loader:
#       # Clone x_batch and set requires_grad=True for the clone.
#       x_batch_clone = x_batch.clone().detach().requires_grad_(True)
# 
#       output = model(x_batch_clone)
#       _, predictions = torch.max(output, 1)
# 
#       model.zero_grad()
# 
#       target = output[torch.arange(x_batch_clone.size(0)), predictions]
#       target.backward(torch.ones_like(target))
# 
#       # Now you can access the gradients from x_batch_clone
#       gradients = x_batch_clone.grad.abs().squeeze()
# 
#       sample_data = x_batch_clone
#       for j in range(len(sample_data)):
#           val = []
#           k_val = pred_test[j] #jth data point, k_val is the prediction of the jth data point
#           # for i in range(40):
#           #     val.append(shap_values[j][i][k_val])
#           val = list(gradients.numpy()[j])
#           samp_x = sample_data[j]
#           # print(samp_x)
#           y = t.reshape((40,1))
#           try:
#             colors = (val/max(val))*256
#             cmap = plt.get_cmap('bwr')  # You can choose any other colormap
#             plt.plot(samp_x.detach().numpy(), t, 'k-', linewidth=2)
#             plt.scatter(samp_x.detach().numpy(), y, c=colors, cmap=cmap, s=100, alpha=0.7)  # Use the specified colormap
#             print("True Label:", int(y_d[j]))
#             print("Model Prediction:", int(k_val))
#             plt.colorbar(label='Color intensity')  # Add colorbar to show mapping
#             plt.title('Scatter Plot with Float Colors')
#             plt.xlabel('X Axis')
#             plt.ylabel('Y Axis')
#             plt.grid(True)
# 
#             # pre_t_val = str(int(y_d[j])) + str(int(k_val))
#             # path = "/content/drive/MyDrive/Saliency_Map_MNIST_results/"
#             # if int(y_d[j]) == int(k_val):
#             #     plt.savefig(path+"CP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
#             # else:
#             #     plt.savefig(path+"WP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
# 
#             # plt.savefig(path+"All/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(c_hidden)+'th_hid_layer.jpg', format='jpg', dpi=300)
# 
#             # pre_t_val = str(int(y_d[j])) + str(int(k_val))
#             # path = '/content/drive/MyDrive/Saliency_Map_MNIST_results/'
#             # if int(y_d[j]) == int(k_val):
#             #     plt.savefig(path+"CP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
#             # else:
#             #     plt.savefig(path+"WP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
#             # plt.savefig(path+"ALL/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
# 
#             # print(path+"All/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(c_hidden)+'th_hid_layer.jpg')
#             # plt.show()
#             # plt.close()
#           except StopIteration:
#             continue
#       break
# 
# 
#

from google.colab import drive
drive.mount('/content/drive')

import os
import re
# Path to your directory

directory_path1 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/CP'
directory_path2 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/WP'

CP_2_22 = []
WP_26_69 = []
CP_70_900 = []


# flexible_before_nn_pattern = r'(.+)_\d+nn'
flexible_before_nn_pattern = r'(.+?)_\d+_\d+nn'

for filename in os.listdir(directory_path1):
    if os.path.isfile(os.path.join(directory_path1, filename)):
      match1 = re.search(r'_(\d+)nn\.jpg', filename)
      hidden_var = int(match1.group(1))
      if hidden_var in list(range(2, 23)):
        flexible_before_nn_match = re.search(flexible_before_nn_pattern, filename)
        flexible_before_nn_string = flexible_before_nn_match.group(1)

        CP_2_22.append(flexible_before_nn_string)
      elif hidden_var in list(range(70, 901)):
        flexible_before_nn_match = re.search(flexible_before_nn_pattern, filename)
        flexible_before_nn_string = flexible_before_nn_match.group(1)

        CP_70_900.append(flexible_before_nn_string)

for filename in os.listdir(directory_path2):
    if os.path.isfile(os.path.join(directory_path2, filename)):
      match1 = re.search(r'_(\d+)nn\.jpg', filename)
      hidden_var = int(match1.group(1))
      if hidden_var in list(range(26, 70)):
        flexible_before_nn_match = re.search(flexible_before_nn_pattern, filename)
        flexible_before_nn_string = flexible_before_nn_match.group(1)

        WP_26_69.append(flexible_before_nn_string)

set(CP_2_22) & set(WP_26_69) & set(CP_70_900)



set(CP_2_22) & set(WP_26_69) & set(CP_70_900)

import os
import re
# Path to your directory

directory_path1 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/CP'
directory_path2 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/WP'

# flexible_before_nn_pattern = r'(.+)_\d+nn'
flexible_before_nn_pattern = r'(.+?)_\d+_\d+nn'
nth_number_pattern = r'(\d+)th_smpl'

for filename in os.listdir(directory_path1):
    if os.path.isfile(os.path.join(directory_path1, filename)):
      match1 = re.search(r'_(\d+)nn\.jpg', filename)
      hidden_var = int(match1.group(1))
      # Search for the adjusted pattern
      nth_number_match = re.search(nth_number_pattern, filename)
      nth_number_string = int(nth_number_match.group(1))

      if (hidden_var in list(range(2, 23)) or hidden_var in list(range(70, 901))) and nth_number_string == 0:
       print(filename)

import os
import re
# Path to your directory

directory_path1 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/CP'
directory_path2 = '/content/drive/MyDrive/Saliency_Map_MNIST_results/WP'

# flexible_before_nn_pattern = r'(.+)_\d+nn'
flexible_before_nn_pattern = r'(.+?)_\d+_\d+nn'
nth_number_pattern = r'(\d+)th_smpl'

for filename in os.listdir(directory_path2):
    if os.path.isfile(os.path.join(directory_path2, filename)):
      match1 = re.search(r'_(\d+)nn\.jpg', filename)
      hidden_var = int(match1.group(1))
      # Search for the adjusted pattern
      nth_number_match = re.search(nth_number_pattern, filename)
      nth_number_string = int(nth_number_match.group(1))

      if hidden_var in list(range(26, 70)) and nth_number_string == 0:
       print(filename)