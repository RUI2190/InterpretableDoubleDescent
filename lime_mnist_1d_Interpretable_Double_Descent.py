# -*- coding: utf-8 -*-
"""Lime + MNIST-1D: Interpretable Double descent

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R74d-OHN9xAgpC8NrA1THM_Grq9FoTw1

# **MNIST-1D**: Observing deep double descent

This notebook investigates double descent as described in section 8.4 of the ["Understanding Deep Learning"](https://udlbook.github.io/udlbook/) textbook.

The deep double descent phenomenon was [originally described here](https://arxiv.org/abs/1812.11118) and later extended to modern architectures and large datasets in an [OpenAI research project](https://openai.com/blog/deep-double-descent/).

This case study is meant to show the convenience and computational savings of working with the low-dimensional MNIST-1D dataset. You can find more details at https://github.com/greydanus/mnist1d.
"""

from google.colab import drive
drive.mount('/content/drive')

# Run this if you're in a Colab to make a local copy of the MNIST 1D repository
!git clone https://github.com/greydanus/mnist1d

!pip install shap

import torch, torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from torch.optim.lr_scheduler import StepLR
import numpy as np
import matplotlib.pyplot as plt
import mnist1d
import random
random.seed(0)

# Try attaching to GPU -- Use "Change Runtime Type to change to GPUT"
DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
print('Using:', DEVICE)

import torch
import matplotlib.pyplot as plt

from mnist1d.data import get_templates, get_dataset_args, get_dataset
from mnist1d.train import get_model_args, train_model
from mnist1d.models import ConvBase, GRUBase, MLPBase, LinearBase
from mnist1d.utils import set_seed, plot_signals, ObjectView, from_pickle

import seaborn as sns
import shap
shap.initjs()

args = mnist1d.data.get_dataset_args()
args.num_samples = 8000 # try after changing this
args.train_split = 0.5 # try after changing this
args.corr_noise_scale = 0.25 # try after changing this
args.iid_noise_scale = 2e-2 # try after changing this
data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=True)

# Add 15% noise to training labels
for c_y in range(len(data['y'])):
    random_number = random.random()
    if random_number < 0.15 :
        random_int = int(random.random() * 10)
        data['y'][c_y] = random_int

# The training and test input and outputs are in
# data['x'], data['y'], data['x_test'], and data['y_test']
print("Examples in training set: {}".format(len(data['y'])))
print("Examples in test set: {}".format(len(data['y_test'])))
print("Length of each example: {}".format(data['x'].shape[-1]))

data

data['t']

data.keys()

for k in range(1,2):
    i = np.random.random_integers(0,100)
    samp_x = data['x'][i]
    t = data['t'].reshape((40,1))
    y = data['y']
    plt.plot(samp_x, t, 'k-', linewidth=2)
    print(y[i])
    plt.show()
    #fig = plot_signals(data['x'], t, labels = y, ratio=3, dark_mode=False)

# Initialize the parameters with He initialization
def weights_init(layer_in):
  if isinstance(layer_in, nn.Linear):
    nn.init.kaiming_uniform_(layer_in.weight)
    layer_in.bias.data.fill_(0.0)

# Return an initialized model with two hidden layers and n_hidden hidden units at each
def get_model(n_hidden):

  D_i = 40    # Input dimensions
  D_k = n_hidden   # Hidden dimensions
  D_o = 10    # Output dimensions

  # Define a model with two hidden layers of size 100
  # And ReLU activations between them
  model = nn.Sequential(
  nn.Linear(D_i, D_k),
  nn.ReLU(),
  nn.Linear(D_k, D_k),
  nn.ReLU(),
  nn.Linear(D_k, D_o))

  # Call the function you just defined
  model.apply(weights_init)

  # Return the model
  return model ;

def fit_model(model, data):

  # choose cross entropy loss function (equation 5.24)
  loss_function = torch.nn.CrossEntropyLoss()
  # construct SGD optimizer and initialize learning rate and momentum
  # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
  optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)


  # create 100 dummy data points and store in data loader class
  x_train = torch.tensor(data['x'].astype('float32'))
  y_train = torch.tensor(data['y'].transpose().astype('long'))
  x_test= torch.tensor(data['x_test'].astype('float32'))
  y_test = torch.tensor(data['y_test'].astype('long'))

  # load the data into a class that creates the batches
  data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))

  # loop over the dataset n_epoch times
  n_epoch = 1000

  for epoch in range(n_epoch):
    # loop over batches
    for i, batch in enumerate(data_loader):
      # retrieve inputs and labels for this batch
      x_batch, y_batch = batch
      # zero the parameter gradients
      optimizer.zero_grad()
      # forward pass -- calculate model output
      pred = model(x_batch)
      # compute the loss
      loss = loss_function(pred, y_batch)
      # backward pass
      loss.backward()
      # SGD update
      optimizer.step()

    # Run whole dataset to get statistics -- normally wouldn't do this
    pred_train = model(x_train)
    pred_test = model(x_test)
    _, predicted_train_class = torch.max(pred_train.data, 1)
    _, predicted_test_class = torch.max(pred_test.data, 1)
    errors_train = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)
    errors_test= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)
    losses_train = loss_function(pred_train, y_train).item()
    losses_test= loss_function(pred_test, y_test).item()
    if epoch%100 ==0 :
      print(f'Epoch {epoch:5d}, train loss {losses_train:.6f}, train error {errors_train:3.2f},  test loss {losses_test:.6f}, test error {errors_test:3.2f}')

  return errors_train, errors_test, predicted_train_class, predicted_test_class

!rm -rf /content/drive/MyDrive/lime/CP

"""#Final Code"""

# Commented out IPython magic to ensure Python compatibility.
!pip install lime
# %load_ext autoreload
# %autoreload 2
from lime import lime_tabular

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from lime import lime_tabular
import numpy as np

num_features = 40
num_classes = 10

# Commented out IPython magic to ensure Python compatibility.
# %%time
# hidden_variables = np.array([2,4,6,8,10,14,18,22,26,30,35,40,45,50,55,60,70,80,90,100,120,140,160,180,200,250,300,400, 500, 600, 700, 800, 900]);
# errors_train_all = np.zeros_like(hidden_variables)
# errors_test_all = np.zeros_like(hidden_variables)
# 
# # For each hidden variable size
# for c_hidden in range(0,len(hidden_variables)):
#     print("#"*100)
#     print(f'Training model with {hidden_variables[c_hidden]:3d} hidden variables')
#     # Get a model
#     model = get_model(hidden_variables[c_hidden]) ;
#     # Train the model
#     errors_train, errors_test, pred_train, pred_test = fit_model(model, data)
#     # Store the results
#     errors_train_all[c_hidden] = errors_train
#     errors_test_all[c_hidden]= errors_test
# 
#     x_train = torch.tensor(data['x'].astype('float32'))
#     y_train = torch.tensor(data['y'].transpose().astype('long'))
#     x_test= torch.tensor(data['x_test'].astype('float32'))
#     y_test = torch.tensor(data['y_test'].astype('long'))
# 
#     #data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))
#     test_data_loader = DataLoader(TensorDataset(x_test,y_test), batch_size=12, shuffle=False, worker_init_fn=np.random.seed(1))#batch_size = 100
# 
#     explainer = lime_tabular.LimeTabularExplainer(
#         training_data=x_train.numpy(),
#         mode='classification',
#         feature_names=['Feature {}'.format(i) for i in range(num_features)],
#         class_names=[f'Class {i}' for i in range(num_classes)],
#         verbose=False,
#         random_state=42
#     )
# 
#     def predict_proba(X):
#       model.eval()
#       with torch.no_grad():
#           tensor = torch.from_numpy(X).float()
#           outputs = model(tensor)
#           probabilities = nn.functional.softmax(outputs, dim=1)
#           return probabilities.numpy()
# 
#     for i, batch in enumerate(test_data_loader):
#       # retrieve inputs and labels for this batch
#       sample_data, y_d = batch #sample_data, y_d = next(iter(test_data_loader))# Get a sample of data for explanation
#       break
# 
# 
#     #Explanation of one correct and one wrong prediction
#     for j in range(len(sample_data)):
#         val = []
#         exp = explainer.explain_instance(
#           sample_data.numpy()[j],
#           predict_proba,
#           num_features=num_features,
#           top_labels=10
#         )
# 
#         exp = exp.as_list()
#         lime_values = []
#         for e in exp:
#             value = e[0].split("Feature ")[1].split(" ")[0]
#             lime_values.append((value, e[1]))
#         sorted(lime_values)
# 
#         k_val = pred_test[j] #jth data point, k_val is the prediction of the jth data point
#         for i in range(40):
#             val.append(lime_values[i][1])
#         val = np.array(val)
# 
#         samp_x = sample_data[j]
#         y = t.reshape((40,1))
#         colors = (val/max(val))*256
#         cmap = plt.get_cmap('bwr')  # You can choose any other colormap
#         plt.plot(samp_x, t, 'k-', linewidth=2)
#         plt.scatter(samp_x, y, c=colors, cmap=cmap, s=100, alpha=0.7)  # Use the specified colormap
#         print("True Label:", int(y_d[j]))
#         print("Model Prediction:", int(k_val))
#         plt.colorbar(label='Color intensity')  # Add colorbar to show mapping
#         plt.title('Scatter Plot with Float Colors')
#         plt.xlabel('X Axis')
#         plt.ylabel('Y Axis')
#         plt.grid(True)
# 
#         pre_t_val = str(int(y_d[j])) + str(int(k_val))
#         path = '/content/drive/MyDrive/lime/'
#         if int(y_d[j]) == int(k_val):
#             plt.savefig(path+"CP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
#         else:
#             plt.savefig(path+"WP/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
# 
#         plt.savefig(path+"ALL/"+str(j)+"th_smpl_"+pre_t_val+"_"+str(hidden_variables[c_hidden])+'nn.jpg', format='jpg', dpi=300)
# 
#         plt.show()
